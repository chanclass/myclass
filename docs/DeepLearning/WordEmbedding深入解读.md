# Word Embedding深入解读

## 介绍

我们在做NLP任务时，经常都会用到词向量。用TensorFlow或者keras等框架时，都会创建一层Embedding层，这个Embedding层可以用**随机生成的矩阵**也可以**预训练的矩阵**。问题是如果最开始使用随机生成的矩阵，经过某个任务训练以后，也就是经过梯度更新了参数，最终的Embedding层的参数会接近之前的预训练的参数吗？或者说的再直白一点，keras的Embedding层与预训练向量的`word vector`有什么区别呢？

回答这个问题，首先只要弄清楚了`word vector`是怎么得到的，就明白了整个过程。网上总结了`Word2vec中数学原理详解`[^1]。使用word2vec训练词向量时，我们虽然没有标记数据，但不是无监督的训练，可以认为是自监督，是利用文本的上下文来训练的，比如`Tomas Mikolov`给出的两个模型，`CBOW`和`Skip-gram`模型，二者的区别在于`CBOW`使用上下文来预测当前词，而`Skip-gram`利用当前词来预测上下文，本质上都是利用神经网络来做分类问题，很明显就是一种监督训练，只不过自己标记了数据。

## 解释监督模型

解释一下为什么是自监督模型：

在语料中截取这样的数据作为训练数据$(context(w_i), w_i)$，分别是上下文和上下文对应的词，目标就是

$$ loss = \sum_{i \in C} \log p(w_i|context(w_i)) $$

让loss最大化，就能得到word2vec的词向量。很明显这就是一个监督问题[^1]吧。

无论是`CBOW`还是`Skip-gram`模型，其结构相同，分别是输入层，投影层，隐藏层，输出层。而在keras或者TensorFlow中的Embedding层**就是一个投影层而已**，如果你学习的目标与word2vec一样，那么你得到的词向量结果基本相同，否则你得到的词向量之间没有想要的相似关系。

## 举例

引用苏剑林的例子（他的文章分析的很透彻[^2]）：

```
我喜欢你
我讨厌你
```

对于这样的文本，利用word2vec训练的结果一定是`喜欢`和`讨厌`距离非常近似，因为上下文是一致的，而在情感分析分析中，这是两种完全不同的情感，那么`喜欢`和`讨厌`这两个词距离相差很远。我之前在做项目时，无论使用预训练词向量还是预训练的模型，其结果都不如随机产生的Embedding层效果好。

## 总结

这里简单总结一下：

1. word2vec是一个完成神经语言模型，包含输入层，投影层，隐藏层，输出层。而Embedding层仅仅只相当于投影层而已。
2. 对于不同的NLP任务来说，虽然都用Embedding层，最终训练得到的副产物词向量（投影层）结果没有任何关联，都是通过更新参数得到的Embedding。
3. 虽然这两个层不是同一个东西，不是说不能使用预训练词向量，还得看具体的任务。在大规模语料中，我们要求词向量有上下文关系的，比如实体识别、机器翻译等任务，预训练的词向量效果会更好，就可以使用Embedding（投影层）加载预训练的词向量矩阵，而这个矩阵仅仅只是一个lookup层而已。

[^1]: 《word2vec数学原理详解》，作者通讯录peghoty@163.com
[^2]: https://spaces.ac.cn/archives/4122
